/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package transactional.consumer;

import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.time.Duration;
import java.util.Collections;
import java.util.Iterator;
import java.util.Map;
import java.util.Properties;
import java.util.Scanner;

import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.OffsetAndMetadata;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;

public class SyncOffsetCommit {
    static String CONSUMER_GROUP_ID = "siemens-sync-cg0";
    //static String TX_ID = "tx-consumer";

    public static void main(String[] args) throws IOException, InterruptedException {
        if (args == null || args.length < 3) {
            System.out.println("SyncOffSetCommit <path/to/properties> <inputTopic> <outputTopic>");
            System.exit(1);
        }
        Properties config = loadConfiguration(args[0]);
        System.out.println("*** Running SyncOffsetCommit ***");
        System.out.println("*** ConfigPath "+args[0]);
        System.out.println("*** InputTopic "+args[1]);
        System.out.println("*** OutputTopic "+args[2]);
        processTransactional(config, args[1], args[2]);
    }

    private static Properties loadConfiguration(final String configFilePath) throws IOException {
        if (!Files.exists(Paths.get(configFilePath))) {
            throw new IOException(configFilePath + " not found.");
        }
        final Properties properties = new Properties();
        try (InputStream inputStream = new FileInputStream(configFilePath)) {
            properties.load(inputStream);
        }

        return properties;
    }

    static void processTransactional(Properties properties, String inputTopic, String outputTopic) throws InterruptedException {
        // We the messages from a topic and process them one by one
        // taking care of the offset commit for each message
        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
        properties.put(ConsumerConfig.GROUP_ID_CONFIG, CONSUMER_GROUP_ID);
        properties.put(ConsumerConfig.CLIENT_ID_CONFIG, "client-1");
        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        properties.put(ConsumerConfig.FETCH_MAX_BYTES_CONFIG, 100);
        //properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, TX_ID);
        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);

        try (Consumer<String, String> consumer = new KafkaConsumer<>(properties);
                Producer<String, String> producer = new KafkaProducer<>(properties);
                Scanner in = new Scanner(System.in);) {

            // Subscribe as consumer
            consumer.subscribe(Collections.singleton(inputTopic));

            try {
                while (true) {
                    ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(10));
                    peekRecords(records);
                    // We ask the user if the want to emulate failure
                    System.out.print("Up to which record you want to process (-1 for all)?");
                    int stopAtRecord = in.nextInt();

                    // Process each record up to the specified
                    Iterator<ConsumerRecord<String, String>> recordsIterator = records.iterator();
                    int currentRecord = 0;
                    while (recordsIterator.hasNext()) {
                        ConsumerRecord<String, String> nextRecord = recordsIterator.next();

                        //process the record - add some prefix and sent to a new topic
                        peekCurrentRecord(nextRecord);
                        String newValue = processRecord(nextRecord);
                        producer.send(new ProducerRecord<String, String>(outputTopic, nextRecord.key(), "Processed-> " + newValue));
                        
                        // Stop Processing?? - Record might be processed but offset not committed
                        if (stopAtRecord >= 0 && currentRecord >= stopAtRecord) {
                            throw new RuntimeException(String.format("Stop processing at %d record", stopAtRecord));
                        }

                        // Handle Offset commit
                        Map<TopicPartition, OffsetAndMetadata> offsetData = Collections.singletonMap(
                                new TopicPartition(nextRecord.topic(), nextRecord.partition()),
                                new OffsetAndMetadata(nextRecord.offset() + 1));
                        consumer.commitSync(offsetData);

                        currentRecord++;
                        Thread.sleep(1000);
                    }
                }
            } catch (RuntimeException e) {
                e.printStackTrace(System.out);
            }
        }
    }

    private static String processRecord(ConsumerRecord<String, String> record) {
        String value = record.value() != null ? record.value().substring(0, Math.min(record.value().length(), 12)) : "[null]";
        return String.format("Origin [T:'%s', P:'%d', O:'%d'] - V:'%s", record.topic(), record.partition(), record.offset(), value);
    }

    static void peekRecords(ConsumerRecords<String, String> records) {
        System.out.printf("Received %d records in the last poll, from %d TopicPartitions%n",
                records.count(),
                records.partitions().size());
                        
    }

    static void peekCurrentRecord(ConsumerRecord<String, String> record) {
        String key = record.key() != null ? record.key().substring(0, Math.min(record.key().length(), 5)) : "[null]";
        String value = record.value() != null ? record.value().substring(0, Math.min(record.value().length(), 12)) : "[null]";
        System.out.printf("Processing record - [T:'%s' P:'%d' O:'%d']-[K:'%-5s' / V:'%-12s']%n",
                record.topic(),
                record.partition(),
                record.offset(),
                key,
                value);
    }
}
